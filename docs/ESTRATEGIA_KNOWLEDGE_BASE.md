# Estrat√©gia para Popular a Tabela `knowledge_base`

## üéØ Objetivo

Popular a tabela `knowledge_base` com an√°lises de alta qualidade geradas por IA para cada um dos 140 territ√≥rios nas 4 dimens√µes, criando uma base de conhecimento rica para alimentar o dashboard e habilitar funcionalidades avan√ßadas como RAG (Retrieval-Augmented Generation).

---

## üìä Situa√ß√£o Atual

- **Tabela:** `knowledge_base`
- **Registros:** 0
- **Impacto:** Cr√≠tico. Sem ela, o sistema n√£o tem "mem√≥ria", as an√°lises s√£o lentas (geradas em tempo real) e funcionalidades de IA avan√ßada s√£o imposs√≠veis.

---

## üìã Estrat√©gia Proposta: Gera√ß√£o H√≠brida (Lote + Sob Demanda)

### **Vis√£o Geral:**

1.  **Gera√ß√£o em Lote (Batch):** Pr√©-processar an√°lises para um subconjunto priorit√°rio de territ√≥rios (ex: os 20 maiores) para garantir performance imediata nos casos de uso mais comuns.
2.  **Gera√ß√£o Sob Demanda com Cache:** Para os demais territ√≥rios, gerar a an√°lise na primeira vez que for solicitada e salv√°-la (cache) na `knowledge_base` para acessos futuros instant√¢neos.
3.  **Gera√ß√£o de Embeddings:** Para cada an√°lise salva, gerar e armazenar um vetor de embedding para habilitar buscas sem√¢nticas (RAG).

### **Vantagens:**
- ‚úÖ **Performance Imediata:** As an√°lises mais importantes estar√£o prontas instantaneamente.
- ‚úÖ **Custo-Benef√≠cio:** Evita o custo de gerar 2.240 an√°lises de uma s√≥ vez, distribuindo o processamento ao longo do tempo.
- ‚úÖ **Escalabilidade:** O sistema se torna mais inteligente a cada uso, completando a base de conhecimento organicamente.
- ‚úÖ **Experi√™ncia do Usu√°rio:** Combina a rapidez do cache com a cobertura completa da gera√ß√£o sob demanda.

---

## üöÄ Passo-a-Passo da Implementa√ß√£o

### **Passo 1: Criar Workflow de Gera√ß√£o em Lote**

**Objetivo:** Automatizar a gera√ß√£o de an√°lises para um grupo de territ√≥rios.

**Ferramenta:** n8n (reaproveitando os agentes especialistas j√° criados).

**Novo Workflow: `WF-BATCH-ANALYSIS-GENERATOR`**

```mermaid
graph TD
    A[Schedule Trigger] --> B{Get Territories};
    B --> C{Loop: For Each Territory};
    C --> D{HTTP: Call ECON Agent};
    D --> E[Save to KB + Embedding];
    C --> F{HTTP: Call SOCIAL Agent};
    F --> G[Save to KB + Embedding];
    C --> H{HTTP: Call TERRA Agent};
    H --> I[Save to KB + Embedding];
    C --> J{HTTP: Call AMBIENT Agent};
    J --> K[Save to KB + Embedding];
```

**L√≥gica do Workflow:**
1.  **Get Territories:** Busca no banco a lista de territ√≥rios a serem processados (ex: `SELECT id, name FROM territories WHERE population > 50000`).
2.  **Loop:** Itera sobre cada territ√≥rio.
3.  **Call Agent:** Para cada dimens√£o, faz uma chamada POST para o webhook do agente especialista correspondente, enviando o `territory_id`.
4.  **Save to KB:** Recebe a an√°lise do agente e a insere na tabela `knowledge_base`.
5.  **Embedding:** Ap√≥s a inser√ß√£o, gera o embedding do texto da an√°lise e o atualiza na tabela.

---

### **Passo 2: Implementar Gera√ß√£o e Cache Sob Demanda**

**Objetivo:** Modificar o fluxo de an√°lise do dashboard para que ele consulte a `knowledge_base` antes de chamar um agente.

**L√≥gica do Backend (Orquestrador):**

```python
def get_analysis(territory_id, dimension):
    # 1. Tenta buscar no cache (knowledge_base)
    analysis = db.query("SELECT content FROM knowledge_base WHERE territory_id = ? AND dimension = ?", (territory_id, dimension))
    
    if analysis:
        # 2. Se encontrou, retorna a an√°lise do cache
        return analysis.content
    else:
        # 3. Se n√£o encontrou, chama o agente especialista
        new_analysis = call_agent(territory_id, dimension)
        
        # 4. Salva a nova an√°lise no cache
        db.execute("INSERT INTO knowledge_base (territory_id, dimension, content) VALUES (?, ?, ?)", (territory_id, dimension, new_analysis))
        
        # 5. Gera e salva o embedding (em background)
        generate_embedding_async(new_analysis)
        
        # 6. Retorna a nova an√°lise
        return new_analysis
```

---

### **Passo 3: Gerar Embeddings para RAG**

**Objetivo:** Transformar cada an√°lise textual em um vetor num√©rico para buscas por similaridade.

**Script:** `generate_embeddings.py` (pode ser um passo no workflow n8n ou um script separado)

```python
import os
import psycopg2
from openai import OpenAI
from pgvector.psycopg2 import register_vector

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Conectar ao DB com pgvector
conn = psycopg2.connect(os.getenv("DATABASE_URL"))
register_vector(conn)
cur = conn.cursor()

# Buscar an√°lises sem embedding
cur.execute("SELECT id, content FROM knowledge_base WHERE embedding IS NULL")
analyses_to_process = cur.fetchall()

for analysis_id, content in analyses_to_process:
    # Gerar embedding com a API da OpenAI
    response = client.embeddings.create(
        input=content,
        model="text-embedding-ada-002" # Modelo padr√£o e de baixo custo
    )
    embedding = response.data[0].embedding
    
    # Salvar o embedding no banco
    cur.execute("UPDATE knowledge_base SET embedding = %s WHERE id = %s", (embedding, analysis_id))
    print(f"‚úì Embedding gerado para a an√°lise {analysis_id}")

conn.commit()
print(f"\n‚úì {len(analyses_to_process)} embeddings gerados e salvos.")
```

---

## ‚è∞ Cronograma e Prioriza√ß√£o

| Fase | Dura√ß√£o | Custo (OpenAI) | Prioridade |
|------|---------|----------------|------------|
| 1. **Batch Top 20** | 1h (setup) + 30min (exec) | ~$0.08 | **ALTA** |
| 2. **Implementar Cache** | 2-3 horas (dev) | $0 | **ALTA** |
| 3. **Batch Completo** | 3-4 horas (exec) | ~$0.56 | M√âDIA |
| 4. **Gerar Embeddings** | 1h (setup) + 1h (exec) | ~$0.02 | M√âDIA |
| 5. **Implementar RAG** | 4-6 horas (dev) | $0 | BAIXA |

**Recomenda√ß√£o:** Come√ßar com a **Fase 1 (Batch Top 20)** e a **Fase 2 (Implementar Cache)**. Isso resolve 80% do problema de performance com 20% do esfor√ßo.

---

## üöÄ Plano de A√ß√£o Imediato

### **1. Criar o Script de Gera√ß√£o em Lote**

**Arquivo:** `batch_generate_analyses.py`

Este script Python servir√° como um prot√≥tipo para o workflow n8n e pode ser usado para a gera√ß√£o inicial.

```python
# batch_generate_analyses.py
import requests
import psycopg2
import os
import time
import json

# Conectar ao DB
conn = psycopg2.connect(os.getenv("DATABASE_URL"))
cur = conn.cursor()

# Buscar os 20 territ√≥rios mais populosos
cur.execute("""
    SELECT t.id, t.name FROM territories t
    JOIN social_indicators si ON t.id = si.territory_id
    WHERE t.type = 'Munic√≠pio' AND si.year = 2023
    ORDER BY si.population DESC
    LIMIT 20;
""")
territories = cur.fetchall()

# Webhooks dos agentes
agents = {
    'economic': 'https://.../agent-econ',
    'social': 'https://.../agent-social',
    'territorial': 'https://.../agent-terra',
    'environmental': 'https://.../agent-ambient'
}

for territory_id, territory_name in territories:
    print(f"\n=== Processando: {territory_name} ===")
    for dimension, webhook_url in agents.items():
        print(f"  Dimens√£o: {dimension}...", end=" ")
        try:
            response = requests.post(webhook_url, json={'territory_id': territory_id, 'analysis_type': 'diagnostic'}, timeout=30)
            if response.status_code == 200:
                analysis = response.json()
                cur.execute("""
                    INSERT INTO knowledge_base (territory_id, dimension, analysis_type, content, metadata)
                    VALUES (%s, %s, %s, %s, %s)
                    ON CONFLICT (territory_id, dimension, analysis_type) DO UPDATE SET
                        content = EXCLUDED.content,
                        updated_at = NOW();
                """, (territory_id, dimension, 'diagnostic', analysis.get('analysis'), json.dumps(analysis.get('metadata', {}))))
                conn.commit()
                print("‚úì An√°lise salva.")
            else:
                print(f"‚úó Erro {response.status_code}")
        except requests.exceptions.RequestException as e:
            print(f"‚úó Erro de conex√£o: {e}")
        time.sleep(2) # Evitar sobrecarga

print(f"\n‚úì An√°lises em lote geradas para {len(territories)} territ√≥rios.")
```

### **2. Executar o Script no Replit**

1.  Salvar o script acima no Replit.
2.  Instalar a depend√™ncia: `pip install requests`.
3.  Definir as URLs corretas dos webhooks dos agentes.
4.  Executar: `python batch_generate_analyses.py`.

---

## üéØ Resultado Esperado

- A tabela `knowledge_base` ser√° populada com aproximadamente 80 an√°lises (20 territ√≥rios √ó 4 dimens√µes).
- O dashboard se tornar√° instantaneamente r√°pido para os 20 munic√≠pios mais importantes.
- Teremos uma prova de conceito funcional para a gera√ß√£o de conhecimento em lote e o sistema de cache.

---

**Framework de Intelig√™ncia Territorial V6.0**  
Henrique M. Ribeiro  
23 de novembro de 2025
